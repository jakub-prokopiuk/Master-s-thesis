\chapter{Wstęp}
We współczesnym świecie dane stanowią jeden z najcenniejszych zasobów przedsiębiorstw, napędzając rozwój systemów informatycznych, analityki biznesowej oraz uczenia maszynowego. Jednakże, wraz ze wzrostem wolumenu przetwarzanych informacji, rośnie również ryzyko związane z ich bezpieczeństwem. W dobie rygorystycznych przepisów o ochronie danych osobowych (RODO/GDPR) \cite{european_parliament_2016_679} oraz rosnącej złożoności systemów informatycznych, pozyskanie realistycznych danych testowych, które zachowują spójność logiczną i kontekstową, stanowi istotne wyzwanie inżynierii oprogramowania.

Tradycyjne podejście do testowania oprogramowania często opierało się na wykorzystaniu kopii produkcyjnych baz danych, poddanych procesom anonimizacji lub maskowania. Metody te, choć skuteczne w zachowaniu struktury danych, niosą ze sobą ryzyko reidentyfikacji osób fizycznych \cite{deanonymization_risks} oraz są trudne do skalowania w środowiskach Continuous Integration/Continuous Deployment (CI/CD). Z drugiej strony, proste generatory danych losowych (ang. random data generators) często tworzą zbiory niespójne semantycznie – np. generując recenzję produktu, która nie ma związku z jego kategorią, czy adresy niepasujące do kraju użytkownika. Taka niespójność utrudnia testowanie zaawansowanych algorytmów analitycznych i systemów rekomendacyjnych.

Przełomem w dziedzinie generowania treści stało się upowszechnienie Dużych Modeli Językowych (LLM, ang. Large Language Models). Modele takie jak GPT-4 czy Llama 3 potrafią generować tekst o wysokim stopniu realizmu, uwzględniając zadany kontekst \cite{llm_synthetic_data}. Jednak samo użycie LLM jest procesem kosztownym obliczeniowo i wolnym, co czyni go niepraktycznym przy generowaniu milionów rekordów bazy danych. Istnieje zatem zapotrzebowanie na rozwiązania hybrydowe, łączące szybkość klasycznych algorytmów z kreatywnością sztucznej inteligencji.

Celem niniejszej pracy jest zaprojektowanie i implementacja narzędzia służącego do proceduralnego generowania relacyjnych zbiorów danych. Proponowane rozwiązanie integruje wydajne algorytmy pseudolosowe (biblioteka Faker) do generowania danych strukturalnych oraz modele LLM do tworzenia danych kontekstowych. System został zaprojektowany w architekturze mikroserwisowej, wykorzystując konteneryzację Docker, kolejkę zadań Redis oraz asynchroniczne przetwarzanie, co pozwala na symulację środowisk produkcyjnych o wysokiej złożoności.